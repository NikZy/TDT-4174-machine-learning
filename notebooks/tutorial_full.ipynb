{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science in Python - A quick tutorial\n",
    "\n",
    "This notebook was created to provide an introduction to data science and \n",
    "machine learning with Python for the 2020 TDT4173 course at NTNU.\n",
    "As with most guides, it is hardly exhaustive, but should \n",
    "hopefully serve as a starting point and/or reference material for \n",
    "those new to the craft. In it, we will cover the following high-level topics:\n",
    "\n",
    "- Numpy fundamentals\n",
    "- Pandas + simple plotting\n",
    "- Skicit Learn + practical example  \n",
    "\n",
    "\n",
    "### Alternatives\n",
    "\n",
    "As there already exists numerous resources on data science in python on the \n",
    "internet, writing this tutorial is somewhat redundant. \n",
    "For any problem or new framework you run into, odds are that there will be \n",
    "a relevant medium, towardsdatascience, or gist blog post just one well-crafted \n",
    "google search away. Kaggle is another good source of knowledge; while primarily\n",
    "a platform for cutting-edge data science competitions, often with cash prizes,\n",
    "they also tend to award points for helpfullness, which over time has resulted\n",
    "in an ever-growing [repository of notebooks](https://www.kaggle.com/notebooks)\n",
    "with helpful explanation of topics. If this tutorial is not \n",
    "to your liking, perhaps one of the following alternatives might be:\n",
    "\n",
    "- [This Kaggle Kernel](https://www.kaggle.com/kanncaa1/data-sciencetutorial-for-beginners) is a great place to start for basic python and data science. In fact, [the author's kaggle page](https://www.kaggle.com/kanncaa1/notebooks) is a highly awarded goldmine of tutorials on various topics in data science and machine learning.\n",
    "- Scikit Learn also have their own set of [tutorials](https://scikit-learn.org/stable/tutorial/index.html) and [guides](https://scikit-learn.org/stable/user_guide.html) on how to use the library.\n",
    "- For those who prefer video tutorials, a youtuber called [Derek Banas](https://www.youtube.com/c/derekbanas/featured) recently put out a set of quick, but reasonably comprehensive tuorials on [numpy](https://www.youtube.com/watch?v=8Y0qQEh7dJg), [pandas](https://www.youtube.com/watch?v=PcvsOaixUh8), [matplotlib](https://www.youtube.com/watch?v=wB9C0Mz9gSo), and [seaborn](https://www.youtube.com/watch?v=6GUZXDef2U0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies  (use \"conda install ...\" if your not using pip)\n",
    "# Note: starting lines with \"!\" in jupyter allows execution of standard\n",
    "# shell functions. The following line has been tested on MAC/Linux,\n",
    "# but consider manual installation if it doesn't work on windows\n",
    "!pip install numpy pandas sklearn matplotlib cmat optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "\n",
    "<img src=\"https://numpy.org/images/logos/numpy.svg\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "[NumPy](https://numpy.org/about/) is the defacto library for scientific computing in Python.\n",
    "Its core datastructures and functionality forms the backbone\n",
    "of many popular libraries such as [Pandas](https://pandas.pydata.org), \n",
    "[Scipy](https://www.scipy.org), \n",
    "[Statsmodels](https://www.statsmodels.org/stable/index.html), \n",
    "[Scikit Learn](https://scikit-learn.org/stable/),\n",
    "[Scikit Image](https://scikit-image.org), and [NLTK](https://www.nltk.org).\n",
    "In addition, its notation and naming conventions are largely reused in frameworks \n",
    "such as [Tensorflow](https://www.tensorflow.org) and [Pytorch](https://pytorch.org). \n",
    "Consequently, it is useful to get familiar with its concepts and build a deep intuition \n",
    "for how it works. For those interested in it from an academic perspective,\n",
    "NumPy recently got a [citable paper in nature](https://www.nature.com/articles/s41586-020-2649-2)\n",
    "(after being actively used in the field for 15 years).\n",
    "This tutorial is more practically oriented and will \n",
    "cover the following topics:\n",
    "\n",
    "- Creating arrays\n",
    "- Vectorized operations\n",
    "- Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the conventional way to import numpy \n",
    "import numpy as np\n",
    "\n",
    "# Set precision when printing out floating point numbers\n",
    "# to make this notebook a bit easier to read \n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays\n",
    "\n",
    "The numpy library revolves around multidimensional \n",
    "arrays, also known as tensors.\n",
    "There are numerous ways to construct these, but a few\n",
    "common ones will be demonstrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest case: from a normal python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 2, 3, 9, 8, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `dtype` argument to explicitly tell numpy\n",
    "which format the data should be in. In this case, we will\n",
    "use 32-bit floating point numbers, which is a format capable\n",
    "of representing decimal numbers (even though we just pass integers here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 2, 3, 9, 8, 7], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make higher-dimensional arrays (higher rank tensor)\n",
    "by passing a nested list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "])\n",
    "print('Shape of matrix:', matrix.shape)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous to the `range()` function in python, numpy arrays\n",
    "can be constructed over a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Simple:', np.arange(5))\n",
    "print('With explicit start, stop, and offset:', np.arange(0, 10, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also built-in constructors for creating a bunch\n",
    "of zeros or ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10 zeros:', np.zeros(10))\n",
    "print('12 ones as a 3x4 matrix: \\n', np.ones((3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even sample from random distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Sampled uniformly in [3,6]: {np.random.uniform(3, 6, size=5)}')\n",
    "print(f'Randomly permuted: {np.random.permutation(5)}')\n",
    "print(f'Randomly sampled from N(3,2) {np.random.normal(3, 2, size=5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Operations\n",
    "\n",
    "A central part of numpy is the idea of vectorized operations.\n",
    "That is, you typically rarely work with individual elements\n",
    "in an array, but instead specify operations over all of them.\n",
    "The cells below give examples of a handful of common vectorized operations\n",
    "and the section is concluded with a demonstration of the performance\n",
    "benefit they provide.\n",
    "\n",
    "**Note:** the vast majority of numpy array operations are immutable, meaning that they will return a new array while leaving the input untouched. Exceptions include operators like the addition assignment (`+=`) and built-in functions that accept an optional `out` keyword (e.g. `np.sin(x, out=x)` will compute sin(x) and write the output back into `x`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most convenional scalar-based arithmetic that works on variables\n",
    "in python will also work on numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "y = x ** 2 + 2 * x\n",
    "print('y = x^2 + 2x: %s -> %s' % (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, there are several built-in vectorized functions \n",
    "that work over all the elements in the array from a single python call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (element-wise) sine function\n",
    "x = np.pi * np.array([0, 0.25, 0.5, 0.75, 1.0], dtype=np.float32)\n",
    "y = np.sin(x)\n",
    "print('y = sin(x): %s -> %s' % (x, y))\n",
    "\n",
    "# (element-wise) exponentiation\n",
    "x = np.array([0, 1, 2, 3], dtype=np.float32)\n",
    "y = np.exp(x)\n",
    "print('y = e^x:    %s -> %s' % (x, y))\n",
    "\n",
    "# (element-wise) natural logarithm\n",
    "x = np.array([ 1., 2.718, 7.389, 20.086], dtype=np.float32)\n",
    "y = np.log(x)\n",
    "print('y = log(x): %s -> %s' % (x, y))\n",
    "\n",
    "# Dot product\n",
    "x = np.array([1, 2], dtype=np.float32)\n",
    "y = np.dot(x, x)  # Alternatively y = x @ x\n",
    "print('y = xTx:    %s -> %s' % (x, y))\n",
    "\n",
    "# Matrix multiplication \n",
    "x = np.array([1, 2, 3], dtype=np.float32)\n",
    "A = np.array([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 0.5, 0.5]\n",
    "])\n",
    "y = np.matmul(A, x)  # Alternatively: y = A @ x\n",
    "print('y = Ax:     %s -> %s' % (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy array also comes with several methods for reduction/aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = x.sum()  # Alternatively: np.sum(x)\n",
    "print('y = sum(x):  %s -> %s' % (x, y))\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "y = x.mean()  # Alternatively: np.mean(x)\n",
    "print('y = mean(x): %s -> %s' % (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For higher dimensional arrays, it is also possible to apply operations over specific\n",
    "axes. E.g. along the columns (row-sums) of a matrix by setting the \n",
    "optional key-word argument `axis=1`. (`axis=0` would sum over columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "y = x.sum(axis=1)  # Alternatively np.sum(x, axis=1)\n",
    "print('y = [sum(x_0), sum(x_1)]: \\n%s -> %s' % (x, y))\n",
    "\n",
    "x = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "y = x.mean(axis=1)  # Alternatively np.mean(x, axis=1)\n",
    "print('\\ny = [mean(x_0), mean(x_1)]: \\n%s -> %s' % (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why vectorized operations are great\n",
    "\n",
    "Aside from convenience, one of the main reasons why\n",
    "we want to use numpy when working with larger sets of \n",
    "data is that it is a lot faster than pure python. As with \n",
    "most interpreted languages, python is relatively slow. On \n",
    "the other hand, despite being a python package, most of the \n",
    "computationally heavy lifting done in numpy is realized by \n",
    "calling compiled, fast C-code behind the scenes. \n",
    "\n",
    "The difference in speed is demonstrated in the example below.\n",
    "The `add_python` function implements a naive way to add together\n",
    "two arrays; by looping over the range of indices and \"manually\"\n",
    "adding elements at each position. The complexity of the number \n",
    "of python statements that has to be executed when this function is\n",
    "called is proportional to the length of the input arrays. \n",
    "In contrast, the `add_numpy` alternative achives the same result\n",
    "with only a single python statement regardless of the length of \n",
    "the input arrays.\n",
    "\n",
    "These two functions are then compared for various array sizes.\n",
    "It can be observed that the pure numpy version quickly \n",
    "dominates the python variant, but the discrepancy grows\n",
    "increasingly large for higher values of n. The key takeaway here\n",
    "is that when working with larger datasets, your python code can\n",
    "receive a substantial speedup by structuring it as vectorized \n",
    "numpy operations as much as possible. Often it is even worth\n",
    "making unnecessary computations that are immediately thrown \n",
    "away just to avoid costly python iteration over an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def add_python(x1, x2):\n",
    "    # Add two arrays and return result in new array using python loop\n",
    "    y = np.empty(len(x1))\n",
    "    for i in range(len(x1)):\n",
    "        y[i] = x1[i] + x2[i]\n",
    "    return y \n",
    "\n",
    "def add_numpy(x1, x2):\n",
    "    # Add two arrays and return result in new array using numpy\n",
    "    return x1 + x2\n",
    "\n",
    "def compare_speed(n):\n",
    "    x1 = np.random.uniform(size=n)\n",
    "    x2 = np.random.uniform(size=n)\n",
    "    # Python\n",
    "    t0 = time.time()\n",
    "    y_py = add_python(x1, x2)\n",
    "    t_py = (time.time() - t0) * 1000 # milliseconds\n",
    "    # Numpy \n",
    "    t0 = time.time()\n",
    "    y_np = add_numpy(x1, x2)\n",
    "    t_np = (time.time() - t0) * 1000 # milliseconds\n",
    "    # Check output matches and compare\n",
    "    np.testing.assert_equal(y_py, y_np)\n",
    "    print('n: %10i | python: %8.4f ms | numpy: %8.4f ms' % (n, t_py, t_np))\n",
    "\n",
    "compare_speed(1)\n",
    "compare_speed(10)\n",
    "compare_speed(100)\n",
    "compare_speed(1000)\n",
    "compare_speed(10000)\n",
    "compare_speed(100000)\n",
    "compare_speed(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencing single values in numpy arrays works the same way \n",
    "as in most other (zero-indexed) languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "print('Element at position 0:', x[0])\n",
    "print('Element at position 1:', x[1])\n",
    "# -1 is the first element starting at the end and going backward\n",
    "print('First element starting at the end and going backwards:', x[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, a list of indices can be passed in to get an array of values out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "ii = [0, 2, -1]\n",
    "print('Elements at positions %s: %s' % (ii, x[ii]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use slices to grab a range of values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "print('Slice from index 3 to (exlusive) 6:', x[3:6])\n",
    "print('Same slice, but different notation:', x[slice(3, 6)])\n",
    "print('Same slice, but but with a stride of 2:', x[3:6:2])\n",
    "print('Same slice, but in reverse:', x[6:3:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even boolean predicates of the same length as the\n",
    "array, in which case  the returned array will only\n",
    "return elements at positions where the predicate holds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "predicate = (x % 2 == 1) & (x > 6) # [F, F, F, F, F, F, T, F, T, F]\n",
    "print('Odd numbers larger than six:', x[predicate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For higher dimensional arrays, the indices along each axis must be separated by commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [ 0,  1,  2,  3],\n",
    "    [ 4,  5,  6,  7],\n",
    "    [ 8,  9, 10, 11]\n",
    "])\n",
    "print('Upper left corner (0,0):', x[0, 0])\n",
    "print('Entire row one:', x[1])\n",
    "print('Entire col three:', x[:, 3])\n",
    "print('First and last (0,0) and (2,3):', x[[0, 2], [0,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/22/Pandas_mark.svg\" alt=\"Drawing\" style=\"width: 160px;\"/>\n",
    "\n",
    "[Pandas](https://pandas.pydata.org) is a popular library for \n",
    "working with, primarily, tabular data (vectors and matrices).\n",
    "It acts as a wrapper around numpy arrays, but adds tons of \n",
    "functionality and convencience for data-science use cases.\n",
    "Its full set of features is far beyond the scope of this tutorial,\n",
    "but is explained in detail in the package's own [documentation](https://pandas.pydata.org/docs/reference/index.html)\n",
    "and [guides](https://pandas.pydata.org/docs/user_guide/index.html).\n",
    "Instead, we will give a preliminary introduction to the following topics:\n",
    "\n",
    "- Series\n",
    "- DataFrames\n",
    "- Indexing\n",
    "- IO\n",
    "- Plotting\n",
    "- Timeseries \n",
    "- Advanced DataFrame Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the conventional way to import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "A pandas series is, in essence, a flat numpy array (vector/rank1 tensor) with an associated index. By default, the index of a series of length N will just be a range [0,N-1], in which case the series will look and feel a lot like a flat numpy array of length N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([3, 4, 5], dtype=np.float32)\n",
    "print('Values as numpy array:', s.values)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a different index can be specified, in which case it may be more appropriate to think about the series as an ordered dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([3, 4, 5], index=['foo', 'bar', 'baz'])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, Pandas works as a wrapper around numpy arrays that adds extra functionality. In fact, it is so integrated with numpy that a Pandas Series (or DataFrame) can be passed directly to a numpy function which in turn will return a pandas object (if appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([3, 4, 5])\n",
    "s = np.exp(s) + 1\n",
    "assert isinstance(s, pd.Series)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "Pandas DataFrames are to matrices as Pandas Series are to vectors. \n",
    "You may want to think about them as python-based spreadsheets.\n",
    "They can be constructed by passing a numpy array (or python list as here)\n",
    "with the desired structure. Columns can also have names. In this case,\n",
    "we pass a list of three strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=[\n",
    "        [1, 3, 5],\n",
    "        [2, 4, 6],\n",
    "    ],\n",
    "    columns=['foo', 'bar', 'baz'],\n",
    "    index=[0, 1], # Same behavior as index in Series (see above)\n",
    ")\n",
    "print('Shape:', df.shape)  # (#rows, #cols) -> think MxN matrix\n",
    "print('Values:'); print(df.values)  # Numpy array with content\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of using DataFrames, as opposed to pure numpy matrices, \n",
    "is that they can (easily) be used to store heterogeneous data types.\n",
    "Here we will create the same DataFrame as in the cell above, except that\n",
    "the first column will hold 32bit integers, the second will hold 32bit floats\n",
    "and the third will hold strings (datatype is O which stands for generic python objects).\n",
    "\n",
    "This DataFrame is built in a different way than the constructor in the previous cell.\n",
    "We start with an empty DataFrame and add columns one by one with \n",
    "the `DataFrame[column_name]` notation. See the \"Indexing\" section\n",
    "below for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['foo'] = np.array([1, 2], dtype=np.int32)\n",
    "df['bar'] = pd.Series([3, 4], dtype=np.float32)\n",
    "df['baz'] = ['five', 'six']\n",
    "print('Data types:', df.dtypes.to_dict()) # df.dtypes returns a series, which can be converted to a dict\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transpose of a DataFrame can also be quickly computed with the `.T` property. Note that may mess up the datatypes, as the columns no longer contains homogenous data. In this case, both were converted to object (O) columns, since it is the most generic datatype that will accept any python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['foo'] = np.array([1, 2], dtype=np.int32)\n",
    "df['bar'] = pd.Series([3, 4], dtype=np.float32)\n",
    "df['baz'] = ['five', 'six']\n",
    "df_transpose = df.T\n",
    "print('Dtypes:', df_transpose.dtypes.to_dict())\n",
    "df_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas ships with lots of convenient functionality for data science.\n",
    "For instance, the `.describe` method generates nice summary statistics\n",
    "of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['foo'] = np.arange(100)\n",
    "df['bar'] = np.random.normal(loc=5, scale=1, size=len(df))\n",
    "df.describe()   # df.count(), df.mean(), df.std(), df.min(), df.quantile(0.25), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series\n",
    "\n",
    "There are two main ways to access data in a Pandas Series; by\n",
    "their index or by their position. Recall that by default, \n",
    "the index of a Series (or DataFrame) of length N will be \n",
    "a range [0, N>, in which case accessing data by index will\n",
    "be equivalent to accessing by position (numpy style).\n",
    "\n",
    "The examples below use the standard array notation ([i]) to \n",
    "access elements. This will access values based on the index,\n",
    "but since the default index is used, it will be equivalent\n",
    "to accessing by position. \n",
    "\n",
    "**Note:** in all the examples below, we use the `.value` \n",
    "property of Series and DataFrames when printing out \n",
    "values. This property returns the content of the Pandas\n",
    "object as a numpy array, which prints more compactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([11, 100, 203])  # index=[0, 1, 2] by default\n",
    "print('Element at index 1:', s[1]) \n",
    "print('Elements from index 1:', s[1:].values)\n",
    "print('Elements at index 0 and 2:', s[[0, 2]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change to a custom index, a string-based on in this case,\n",
    "we can observe that access with array-notation does in fact \n",
    "use the index, and not the position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([11, 100, 203], index=['a', 'b', 'c'])\n",
    "print('Element at index \"a\":', s[\"a\"])\n",
    "print('Element at index \"a\" and \"c\":', s[[\"a\", \"c\"]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove any ambiguity around whether index or position is being referenced,\n",
    "you can use the `.loc[i]` property of a series. This accessor\n",
    "behaves the same way as `[i]`, but is more explicit\n",
    "and therefore often perceived as less ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([11, 100, 203], index=['a', 'b', 'c'])\n",
    "print('Element at index \"a\":', s.loc[\"a\"])\n",
    "print('Element at index \"a\" and \"c\":', s.loc[[\"a\", \"c\"]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** note that `.loc[i:j]` slices define an open \n",
    "interval, meaning that, unlike standard array notation, it\n",
    "will return elements up to **and including** `j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([0, 1, 2, 3, 4])\n",
    "print('Slice with s[2:4]:', s[2:4].values)\n",
    "print('Slice with s.loc[2:4]:', s.loc[2:4].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, elements in the series can explicitly be accessed by\n",
    "position, regardless of the index used, by using the `.iloc[i]` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([11, 100, 203], index=['a', 'b', 'c'])\n",
    "print('Element at position 1:', s.iloc[1])\n",
    "print('Elements from position 1:', s.iloc[1:].values)\n",
    "print('Elements at position 0 and 2:', s.iloc[[0, 2]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to numpy arrays, passing a boolean predicate array\n",
    "of the same size as the series will return the element where\n",
    "the predicate holds true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([11, 100, 203], index=['a', 'b', 'c'])\n",
    "predicate = s % 2 == 1\n",
    "print('Odd elements in the series:', s[predicate].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing/positional accessors can also be used to set values.\n",
    "In this example, we change the value of the element at position\n",
    "\"a\" from 11 to 0, and then add a new element (indexed by \"w\")\n",
    "to the end with the value of 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([11, 100, 203], index=['a', 'b', 'c'])\n",
    "print('Before %s' % s.values)\n",
    "s['a'] = 0\n",
    "print('After setting \"a\" to 0:', s.values)\n",
    "s['w'] = 99\n",
    "print('After adding new element \"w\"=99:', s.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to Series, DataFrames are 2-dimensional (row, column),\n",
    "which leads to a slightly different syntax. To access columns,\n",
    "the following expressions lead to the same result when trying to\n",
    "get the column \"foo\" (as a Series) from a DataFrame:\n",
    "\n",
    "- `df.foo` While convenient to write, this only works for columns whose name follows \n",
    "    the same rules as standard python variables and does not conflict \n",
    "    with any of the DataFrame's attributes or methods (e.g. `sum`, `count`, etc)\n",
    "- `df['foo']` This is arguably the most common way to access columns.\n",
    "    It also works for all possible column names.\n",
    "- `df.loc[:,'foo']` This essentially says; \"take everything along the\n",
    "    first axis (rows) and 'foo' along the second axis (cols)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'foo': [1, 2, 3], 'bar': [9, 8, 7]})\n",
    "print('Getting the foo column with df.foo:', df.foo.values)\n",
    "print('Is equivalent to using df[\"foo\"]:', df[\"foo\"].values)\n",
    "print('Which is the same as df.loc[:, \"foo\"]:', df.loc[:, \"foo\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to Series, data at individual rows can be \n",
    "fetched from DataFrames with the `.loc[i]` notation. \n",
    "Note that since there can be multiple values in each row,\n",
    "this accessor will return a Series of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'foo': [1, 2, 3], 'bar': [9, 8, 7]}, index=['a', 'b', 'c'])\n",
    "print('Row at index \"a\":', df.loc[\"a\"].values)\n",
    "print('Rows at index \"a\" and \"c\":')\n",
    "df.loc[[\"a\", \"c\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of `.iloc[i]` is also what you might expect.\n",
    "It will return element(s) at the given positions as if \n",
    "you were indexing a raw numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'foo': [1, 2, 3], 'bar': [9, 8, 7]}, index=['a', 'b', 'c'])\n",
    "print('Elements in row at position 0:', df.iloc[0].values)\n",
    "print('Elements in rows at positions 0 and 1')\n",
    "df.iloc[[0, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat counterintuitively, while the `[i]` accessor on a \n",
    "DataFrame will slice the dataframe along the second axis (columns) \n",
    "if you pass a column name (or list of column names), passing\n",
    "a boolean predicate array will make it behave like a Series\n",
    "and select elements along the first axis (rows). Note: you can always\n",
    "use the `.loc[i]` accessor to make this behavior more explicit\n",
    "and intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'foo': [1, 2, 3], 'bar': [9, 8, 7]})\n",
    "predicate = df.bar % 2 == 1\n",
    "assert df.loc[predicate].equals(df[predicate])\n",
    "print('Elements where column \"bar\" is odd')\n",
    "df[predicate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to a Series, you can use accessors to write to \n",
    "a DataFrame. The one exception is the dot notation\n",
    "(`.foo` in the example above), which can only be used \n",
    "to reference existing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'foo': [1, 2, 3], 'bar': [9, 8, 7]})\n",
    "# Add new column \"baz\" as we have seen previously\n",
    "# df.baz = [5, 5, 5]  # <--- This will not work (but not raise an exception)\n",
    "df['baz'] = [5, 5, 5]  # <--- This works just fine\n",
    "# Change the last value in the 'bar' column\n",
    "df.loc[2, 'bar'] = 999\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IO\n",
    "\n",
    "Another great perk of using Pandas is its extensive support for \n",
    "[different data formats](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) \n",
    "that can be read from and written to.\n",
    "The standard convention used by the API is `DataFrame.to_<format>(file, ...)`\n",
    "when writing and `pd.from_<format>(file, ...)` when reading,\n",
    "where `file` is either a filepath, url, or a python file object\n",
    "returned by the `open(...)` function.\n",
    "Some formats, like _csv_ in the example below, allow the file argument\n",
    "to be completely omitted, in which case the function will write to \n",
    "and return a python string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1, 3], [2, 4]], columns=['foo', 'bar'])\n",
    "csv_string = df.to_csv()\n",
    "print(csv_string)\n",
    "# Trick to write csv_string to something that looks like a file object\n",
    "import io; buf = io.StringIO(); buf.write(csv_string); buf.seek(0)\n",
    "# Then read it back out with Pandas\n",
    "pd.read_csv(buf, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas package also comes with a lot of useful\n",
    "built-in plotting functionality built on the [matplotlib](https://matplotlib.org)\n",
    "library. However, if you want your plots to really pop\n",
    "with minimum amount of extra work, then the [seaborn](http://seaborn.pydata.org) package\n",
    "is a good resource that integrates really well with Pandas\n",
    "and matplotlib. The cells below show examples using \n",
    "both libraries.\n",
    "\n",
    "**Note:** The plotting functions in the examples below\n",
    "all rely on the matplotlib package in the background.\n",
    "We also explicitly use the function \n",
    "`matplotlib.pyplot.subplots(figsize=(W,H), nrows=M, ncols=N)`\n",
    "to generate an MxN grid if plot surfaces to draw on.\n",
    "The plotting functions used actually don't require this,\n",
    "but it allows us to easily position the plots by telling \n",
    "the plotting functions where to draw with the `ax` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the conventional way to import matplotlib.pyplot and seaborn\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plot\n",
    "\n",
    "Here we fill a DataFrame with two series, each being sampled\n",
    "independently from two different normal distributions.\n",
    "The two columns are then visualized against each other\n",
    "in a 2D scatterplot, with the pandas-based one to the \n",
    "left and the seaborn-based on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill a data frame with two series sampled from different normal distributions\n",
    "df = pd.DataFrame({'x': np.random.normal(3, 1, size=1000), 'y': np.random.normal(2, 2, size=1000)})\n",
    "# Generate figure with two side-by-side plotting areas\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(16,5), ncols=2)\n",
    "# And plot with pandas and seaborn\n",
    "df.plot.scatter(x='x', y='y', ax=ax1)\n",
    "sns.scatterplot(x='x', y='y', data=df, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram\n",
    "\n",
    "Next we will construct histograms from a single Series of values\n",
    "randomly sampled from a Normal distribution. Again, the pandas-based\n",
    "plot is on the left while the seaborn-based on is on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill a series with IID normal values \n",
    "s = pd.Series(np.random.normal(size=(1000)))\n",
    "# Generate figure with two side-by-side plotting areas\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(16,5), ncols=2)\n",
    "# And plot with pandas and seaborn\n",
    "s.plot.hist(ax=ax1, bins=20)\n",
    "sns.distplot(s, ax=ax2, bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lineplot\n",
    "\n",
    "Finally, we'll make a lineplot. To demonstrate this we generate\n",
    "10 trajectories of brownian motion, drawing increments from\n",
    "a standard normal distribution. The seaborn-based plot is \n",
    "also generated slightly differently, Instead of plotting \n",
    "each path individually, we plot the average value across\n",
    "all paths with the (sample) standard deviation visualized\n",
    "with a shaded area. \n",
    "\n",
    "To use the seaborn lineplot, the data\n",
    "frame needs to be in a slighly different format. Instead\n",
    "of having N columns [var1, var2, ..., varN] and M rows, it \n",
    "requires 2 columns [index, value] and N*M rows. This is \n",
    "easily achieved with the `DataFrame.melt` method. \n",
    "The melt method also generates a column storing the name\n",
    "of the column each datapoint was sourced from (variable).\n",
    "To make the seaborn-based plot equivalent to the one \n",
    "generated with pandas, we could have pased `hue='variable'`\n",
    "to the plot function, which would have made it separate the\n",
    "lines based on the elements in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill a dataframe with 10 brownian motion paths\n",
    "df = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    # var_i(t) = var_i(t-1) + e; var_i(0) = 0, e ~ N(0, 1) \n",
    "    df[f'var_{i}'] = np.random.normal(size=100).cumsum()\n",
    "# Generate figure with two side-by-side plotting areas\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(16,5), ncols=2)\n",
    "# Plot each column with built-in plot function\n",
    "df.plot(ax=ax1)\n",
    "# Plot mean value with +/ stddev intervals\n",
    "df_melted = df.reset_index().melt(id_vars='index')\n",
    "sns.lineplot(x='index', y='value', ci='sd', data=df_melted);  # add hue='variable' to get same plot as with df.plot\n",
    "# Grab the 3 first values in the melted data frame to show what it looks like\n",
    "df_melted.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before wrapping up the Pandas section we'll provide short\n",
    "demonstration of how it can be helpful when working with\n",
    "timeseries data. The DataFrame (or Series) index can take \n",
    "on datetime values, which allows for certain operations including:\n",
    "\n",
    "- Selecting from or slicing the Series/DataFrame by date.\n",
    "- Resampling the data to different convenient intervals.\n",
    "\n",
    "The examples below illustrates how this can be done and \n",
    "visualizes the results in different plots. Note also that\n",
    "whenever the index is a datetime object, the built-in\n",
    "Pandas lineplot will make sure that the dates are nicely\n",
    "printed along the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a datetime index 100 steps spaced 1 Day appart starting at year 2000\n",
    "dt_index = pd.date_range(start='2000-01-01', freq='1D', periods=100)\n",
    "# Initialize a dataframe using this index\n",
    "df = pd.DataFrame(index=dt_index)\n",
    "# Fill it with some brownian motion\n",
    "for i in range(5):\n",
    "    df[f'var_{i}'] = np.random.normal(size=len(df)).cumsum()\n",
    "    \n",
    "# Generate figure with three stacked plotting areas\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(figsize=(12, 10), nrows=3)\n",
    "# Plot each column with built-in plot function\n",
    "df.plot(ax=ax1, title='100 days of brownian motion')\n",
    "\n",
    "# Slice out just the month of February 2000 and plot it\n",
    "# Note: since we're using look, it will actually include March 1st\n",
    "df_slc = df.loc['2000-02-01':'2000-03-01']\n",
    "df_slc.plot(ax=ax2, title='Just February')\n",
    "\n",
    "# Resample time series from day resolution to mean and take average value \n",
    "df_resamp = df.resample('1W').mean()\n",
    "df_resamp.plot(ax=ax3, title='Resampled to week resolution')\n",
    "\n",
    "# Automatically adjust plot to avoid overlapping elements\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced DataFrame Manipulation\n",
    "\n",
    "Pandas has become one of the most popular tools to perform Explanatory Data Analysis on real data. \n",
    "The best way to demonstarate it's abilities is to operate with some prepared dataset. For this part we will use NBA games data from https://github.com/fivethirtyeight/data/tree/master/nba-elo.\n",
    "\n",
    "The first trick is that pandas can read files not only from your local filesystem, but also from a URL. You can directly pass the URL instead of a file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/fivethirtyeight/data/master/nba-elo/nbaallelo.csv\"\n",
    "nba_dataset = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.head(n)` method returns the first `n` rows (same as `.iloc[:n]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_dataset.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `.info()` method returns metadata about the dataframe that might be useful for diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime Parsing\n",
    "\n",
    "As you can see, the \"date_game\" column has been parsed wrongly. As it is an ordinar python object, we have to parse this data and convert it into datetime object. There are two main ways of how to do that:\n",
    "\n",
    " 1. Provide a parser function when reading the dataset\n",
    " 1. Parse it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st way\n",
    "# date_parser = lambda x: pd.datetime.strptime(x, '%m/%d/%Y')\n",
    "# nba_dataset = pd.read_csv(url, parse_dates=['date_game'], date_parser=date_parser)\n",
    "\n",
    "# 2nd way (overwrite old date_game column with a parsed version)\n",
    "nba_dataset['date_game'] = pd.to_datetime(nba_dataset['date_game'], dayfirst=False, yearfirst=False)\n",
    "\n",
    "nba_dataset.info()  # Observe that \"date_game\" now is of type datetime64[ns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Values\n",
    "\n",
    "The `.nunique()` method returns the number of unique values in a\n",
    "column (Series), the `.unique()` method returns the unique values,\n",
    "and the `.value_counts()` method returns a Series with the count \n",
    "of every unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate how many games are in the dataset \n",
    "print('Num games:', nba_dataset['game_id'].nunique())\n",
    "# What leagues are presented in the dataset?\n",
    "print('Leagues:', nba_dataset['lg_id'].unique())\n",
    "# How many games performed in every league?\n",
    "# Note: there are multiple entries for some games,\n",
    "# so we filter the dataframe to only those rows \n",
    "# where '_iscopy' is False (zero)\n",
    "print('Num games in each league:')\n",
    "print(nba_dataset[nba_dataset['_iscopy'] == 0]['lg_id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting\n",
    "\n",
    "The `.sort_values(...)` method can be used to sort data by one or more columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sorting\n",
    "nba_dataset.sort_values('date_game', ascending=False)\n",
    "\n",
    "# Sorting on several columns. Note that direction is specified for each column\n",
    "nba_dataset.sort_values(['date_game', 'pts'], ascending=[False, True]).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping\n",
    "\n",
    "A common use case when analyzing data is to group it by\n",
    "one or more variables and apply operations on each group\n",
    "separately. A typical grouping operation looks like the \n",
    "following:\n",
    "\n",
    "`df.groupby(by=grouping_columns)[columns_to_show].function()`\n",
    "\n",
    "- The groupby method creates a partitioning based on unique values in the grouping_columns\n",
    "- We select the columns we need (columns_to_show).\n",
    "- A function or multiple functions are applied to the resulting groups.\n",
    "\n",
    "In the example below, the sum of points (pts) is calculated \n",
    "for each team (fran_id). Not that in the resulting DataFrame,\n",
    "fran_id becomes the new index whereas pts becomes the only\n",
    "column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a \"group_by\" object around the team column (fran_id)\n",
    "grouper = nba_dataset.groupby('fran_id')\n",
    "# Then we extract the team and points columns\n",
    "grouper = grouper[['fran_id', 'pts']]\n",
    "# Then we apply the sum function \n",
    "group_sums = grouper.sum()\n",
    "# And finally sort the rows by decreasing number of points\n",
    "group_sums = group_sums.sort_values('pts', ascending=False)\n",
    "\n",
    "# As shown above, this can be done in one line by chaining methods and accessors\n",
    "group_sums = nba_dataset.groupby('fran_id')[['fran_id', 'pts']].sum().sort_values('pts', ascending=False)\n",
    "group_sums.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot Tables\n",
    "\n",
    "Pivot tables are useful for breaking down and describing a \n",
    "variable with respect to many others. Those familiar with\n",
    "spreadsheet programs might already be familiar with the concept.\n",
    "Intuitively, it can be tought of as a groupby operation along \n",
    "both the row and column axis. Here we will examine matches \n",
    "between (two of) three teams and break down the average \n",
    "number of points earned with respect to team, game result, and \n",
    "the year the game was played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we filter the data to only those games where\n",
    "# the Lakers, Warriors, or Celtics were involved\n",
    "teams = ['Lakers', 'Warriors', 'Celtics']\n",
    "predicate = nba_dataset['fran_id'].isin(teams) & nba_dataset['opp_fran'].isin(teams)\n",
    "small_df = nba_dataset[predicate]\n",
    "\n",
    "# Then we extract columns for team, year, result and number of points\n",
    "small_df = small_df[['fran_id', 'year_id', 'game_result', 'pts']]\n",
    "\n",
    "# And construct a pivot table \n",
    "nba_pt = pd.pivot_table(\n",
    "    small_df,\n",
    "    # Break down by team and result along the row axis\n",
    "    index=['fran_id', 'game_result'],\n",
    "    # Break down by year along the column axis\n",
    "    columns=['year_id'],\n",
    "    # Average values in body of the table (points)\n",
    "    aggfunc='mean'\n",
    ")\n",
    "nba_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking and Unstacking\n",
    "\n",
    "Note that in the pivot table above, the index has become a \n",
    "hierarchical tuple (fran_id, game_result) and the columns \n",
    "a hierarchical tuple (pts, year_id). Under \n",
    "the hood, indexes and columns are the exact same data\n",
    "structure, just used to describe the data along the \n",
    "first and second axis respectively. Consequently it is \n",
    "straightforward to swap values in the two data \n",
    "structures around. The `.stack(level)` method will reorganize\n",
    "the dataframe so that the column(s) specified by `level` are\n",
    "moved to the index.\n",
    "\n",
    "**Note:** In the examples below, we always refers to column\n",
    "or index \"levels\" by their string name. This is possible because\n",
    "the `pivot_table` function generated indexes and columns with \n",
    "named levels. In DataFrames where this is not the case, you \n",
    "can still use the `stack` functions by referring to levels\n",
    "with an integer (see the comments in the cells below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nba_pt.stack('year_id')  # Alternatively nba_pt.stack(1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding a call to the `.reset_index()` method, we can\n",
    "convert the (fran_id, game_result, year_id) values of the\n",
    "hierarcical index to normal columns, essentially reverting\n",
    "to the \"small_df\" variable used to construct the pivot table\n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nba_pt.stack('year_id').reset_index()  # Alternatively nba_pt.stack(1).reset_index()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.unstack(indexes)` method works in the \n",
    "oposite way, and converts indexes to columns.\n",
    "Here we will use it in conjuction with `stack`\n",
    "to swap \"year_id\" (columns) with \"game_result\"\n",
    "(second level of index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nba_pt.stack('year_id').unstack('game_result')  # Alternatively: nba_pt.stack(1).unstack(1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating\n",
    "\n",
    "Another relatively common use case is to combine two or\n",
    "more DataFrames or Series into a single one. The `concat`\n",
    "function concatenates a list of DataFrames or Series \n",
    "toghether, either vertically (along the row (0) axis)\n",
    "or horizontally (along the column (1) axis).\n",
    "\n",
    "**Note:** We'll also take this opportunity to introduce \n",
    "the `query` method of DataFrames. This is just a more \n",
    "elegant way to filter the rows of a dataframe based on\n",
    "the values in one or more columns. That is, `df.query('foo == x')`\n",
    "is equivalent to `df[df.foo == x]`. Also, the method allows\n",
    "the query string to reference variables from the current \n",
    "scope by prepending the variable name by `@`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset by team=celtics and then team=lakers\n",
    "team2 = 'Lakers'\n",
    "df_1 = nba_dataset.query('fran_id == \"Celtics\"')[['fran_id', 'pts']]\n",
    "df_2 = nba_dataset.query('fran_id == @team2')[['fran_id', 'pts']]\n",
    "# Concatenate vertically (axis=1 would result in horizontal concatination)\n",
    "df_both = pd.concat([df_1, df_2], axis=0)\n",
    "df_both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging\n",
    "\n",
    "Pandas has full-featured in-memory join operations, idiomatically very similar to SQL-based relational databases. Here, we'll demonstrate how a left join can be carried out with the `merge` function.\n",
    "\n",
    "First, we grab the game_id, year, team, and points scored\n",
    "for each datapoint in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_team = nba_dataset.query('_iscopy != 0')[['game_id', 'year_id', 'fran_id', 'pts']]\n",
    "df_date_team.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the yearly average number of points of each team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avgp_per_year = (nba_dataset\n",
    "    .query('_iscopy != 0')[['year_id', 'fran_id', 'pts']]\n",
    "    .groupby(['year_id', 'fran_id']).mean()\n",
    "    .reset_index()\n",
    ")\n",
    "df_avgp_per_year.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the yearly averages back into the first\n",
    "DataFrame. We use a left join, meaning that for \n",
    "each (year_id, fran_id) tuple in the first (left) data\n",
    "frame, the function will look up the corresponding \n",
    "(year_id, fran_id) in the second (right) data frame and concatenate\n",
    "the result horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    # Merge with the first dataframe on the left and \n",
    "    # the second (yearly averages) on the right\n",
    "    left=df_date_team,\n",
    "    right=df_avgp_per_year,\n",
    "    # Look up values in the right based on values in the left \n",
    "    how='left',\n",
    "    # Using the following keys (same names in both dataframes)\n",
    "    left_on=['year_id', 'fran_id'], \n",
    "    right_on=['year_id', 'fran_id'],\n",
    "    # Keep the columns in the left dataframe unchanged,\n",
    "    # but add an '_avg' suffix to columns in the right\n",
    "    suffixes=('', '_avg')\n",
    ")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful when choosing the merging method. You may recieve different results based on the four types:\n",
    "\n",
    "- left — Use keys from left frame only\n",
    "- right — Use keys from right frame only\n",
    "- outer — Use union of keys from both frames\n",
    "- inner — Use intersection of keys from both frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "[Scikit Learn](https://scikit-learn.org/stable/) is a python library\n",
    "that provides stable implementations of a wide selection of \n",
    "machine learning algorithms. Additionally, it provides tools for \n",
    "preprocessing, analysis, and evaluation, as well\n",
    "as a framework setting up machine learning workflows. Except for simple\n",
    "architectures (e.g. [MLP](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)),\n",
    "this library is not the best resource for neural networks and \n",
    "deep learning. For those interested in those topics, libraries such as\n",
    "[PyTorch](https://pytorch.org) and [Tensorflow](https://www.tensorflow.org)\n",
    "are likely to provide more utility. Nevertheless, many of the tools provided by Scikit\n",
    "Learn can still be useful in a predominantly deep-learning-based setting,\n",
    "and the classical machine learning algorithms provided\n",
    "often serve well as baselines when justifying more complex approaches.\n",
    "\n",
    "The rest of this tutorial will cover the following:\n",
    "\n",
    "- The Estimator Class\n",
    "- Metrics\n",
    "- A practical classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Estimator Class\n",
    "\n",
    "Most components in the Scikit Learn package implements a \n",
    "common interface. For supervised predictive models, the\n",
    "three most relevant methods are:\n",
    "\n",
    "- **fit(X, y)**: which fits the model's parameters so that\n",
    "    it approximates $f : X \\rightarrow y$, where X is (usually) an \n",
    "    MxN-dimensional matrix and y is an M-dimensional vector.\n",
    "    The exact datatype and expected values in X and y depends\n",
    "    on the machine learning method used. Also note that for \n",
    "    unsupervised methods (e.g. KMeans), only the X matters,\n",
    "    so the y is typically optional and will be ignored if passed.\n",
    "    \n",
    "- **predict(X)**: which makes a prediction with the model over \n",
    "    the MxN-dimensional input matrix (note that M doesn't have\n",
    "    to be the same here as in fit). Naturally, this method should\n",
    "    only be called after the .fit method has been called.\n",
    "    \n",
    "- **score(X, y, [sample_weight])**: which evaluates the model over\n",
    "    a set of datapoints. The evaluation measure used \n",
    "    varies from model to model. For instance, most classifiers\n",
    "    tend to use accuracy (see next section).\n",
    "    \n",
    "The set of *hyperparameters*, i.e. all model parameters that are \n",
    "not inferred from data in the .fit method, are passed as arguments\n",
    "to the model's constructor function. Behind the scenes, the base\n",
    "class used for all estimators (sklearn.base.BaseEstimator) also\n",
    "implements the methods `.get_params()` and `.set_params(...)`.\n",
    "The values returned by the former will be based on the arguments \n",
    "passed to the constructor and the latter allows these parameters\n",
    "to be overridden on an already constructed model.\n",
    "\n",
    "Other methods that you might encounter include:\n",
    "\n",
    "- **predict_proba(X)**: which returns the associated probabilities/confidence \n",
    "    scores for each prediction. These are most commonly found in classifier\n",
    "    models, in which case they will return either just a vector of probabilities\n",
    "    $p; p_i \\in [0, 1]$ for binary classification or a matrix of \n",
    "    probabilities $P$ where all the rows sum to one.\n",
    "- **transform(X)**: which does not make a prediction, but instead transforms\n",
    "    input X to an alternative representation. This method is commonly implemented\n",
    "    in preprocessing models such as [scalers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "- **fit_transform(X, y)**: similar to transform, except that it first calls \n",
    "    fit and then transforms the input.\n",
    "    \n",
    "More advanced models include composit models that encapsulate other models.\n",
    "A good example of this is the [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html),\n",
    "which permits chaining together different preprocessing models,\n",
    "typically with a predictive model at the top. \n",
    "\n",
    "For simplicity, we'll focus on predictive models in this tutorial,\n",
    "starting with a custom implementation of [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "The cell below implements the three basic methods mentioned above, which\n",
    "provides the bare minimum for estimating, using, and evaluating\n",
    "a machine learning model. In addition there is a constructor that \n",
    "accepts a single parameter `use_bias`, which flags whether an \n",
    "intercept will be fitted when estimating parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.base\n",
    "\n",
    "class LinearRegressor(sklearn.base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    A simple, custom linear regression model that is highly similar\n",
    "    to sklearn.linear_model.LinearRegression\n",
    "    \n",
    "    Args:\n",
    "        use_bias: whether to also fit the intercept of the hyperplane\n",
    "            when estimating coefficients.\n",
    "    \"\"\"\n",
    "    \n",
    "    # By convention, attributes that are generated when the model\n",
    "    # is fitted to data are stored under properties called \"<name>_\"\n",
    "    weights_ = None\n",
    "    bias_ = None \n",
    "    residuals_ = None\n",
    "    \n",
    "    def __init__(self, use_bias=True):\n",
    "        \"\"\"\n",
    "        Initializes the estimator.\n",
    "        \"\"\"\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the estimator to the data.\n",
    "        \n",
    "        Args:\n",
    "            X: an MxN-diensional matrix of real-valued features\n",
    "            y: an M-dimensional vector of real-valued target outputs\n",
    "            \n",
    "        Returns:\n",
    "            A reference to this estimator, but with fitted parameters.\n",
    "        \"\"\"\n",
    "        # Make sure it is an array, in case we get a list or DataFrame\n",
    "        X = np.asarray(X)\n",
    "        # Augment matrix with a column of ones if bias \n",
    "        if self.use_bias:\n",
    "            X = np.concatenate([X, np.ones((len(X), 1))], axis=1)\n",
    "        # Estimate coefficients with least square \n",
    "        coef, res, *_ = np.linalg.lstsq(X, y, rcond=-1)\n",
    "        if self.use_bias:\n",
    "            weights, bias = coef[:-1], coef[-1]\n",
    "        else:\n",
    "            weights, bias = coef, 0 \n",
    "        # Set state \n",
    "        self.weights_ = weights\n",
    "        self.bias_ = bias \n",
    "        self.residuals_ = res \n",
    "        # And, by convention, return the train predictor (self in this case)\n",
    "        return self \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make a prediction with trained parameters\n",
    "        \n",
    "        Args:\n",
    "            X: an MxN-dimensional matrix of real-valued features\n",
    "            \n",
    "        Returns:\n",
    "            An N-dimensional vector of real-valued predictions\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        # Make sure that weights have been calculated with .fit()\n",
    "        assert self.weights_ is not None\n",
    "        # Make sure X has the right shape\n",
    "        assert X.shape[1] == self.weights_.shape[0]\n",
    "        # Return y_hat = Xw + b  ; where b might be zero if use_bias=False\n",
    "        return X @ self.weights_ + self.bias_\n",
    "    \n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Scores the models over a set of datapoints using negative mean squared error\n",
    "        \n",
    "        Args:\n",
    "            X: an MxN-dimensional matrix of real-valued features\n",
    "            y: an M-dimensional vector of real-valued target outputs\n",
    "            sample_weight: a string with the name of the scoring function to use\n",
    "            \n",
    "        Returns:\n",
    "            A real-valued scalar equal to the negative mean squared error\n",
    "        \"\"\"\n",
    "        y_hat = self.predict(X)\n",
    "        squared_errors = (y_hat - y) ** 2\n",
    "        # Compute weighted average of weights. Note that if \n",
    "        # sample_weight is None, this function will just \n",
    "        # compute a simple, unweighted average \n",
    "        # ALSO: note that we return the negative MSE! \n",
    "        # This is a convention in sklearn where all scoring\n",
    "        # functions are defined so that HIGHER -> BETTER\n",
    "        # All error-based scoring measures thus have to be negated\n",
    "        return -np.average(squared_errors, weights=sample_weight)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, true_weights, true_bias, error_variance):\n",
    "    true_weights = np.array(true_weights)\n",
    "    X = np.random.uniform(0, 10, size=(n, len(true_weights))) \n",
    "    e = np.random.normal(0, np.sqrt(error_variance), size=n)\n",
    "    y = X @ true_weights + true_bias + e\n",
    "    return X, y\n",
    "    \n",
    "    \n",
    "X, y = generate_data(n=10000, true_weights=[-1.3, 0.1, 2.1], true_bias=3, error_variance=2)\n",
    "reg = LinearRegressor(use_bias=True)\n",
    "reg.fit(X, y)\n",
    "print('Estimated weights=%s bias=%.3f' % (reg.weights_, reg.bias_))\n",
    "print('In-sample score (negative MSE): %.3f' % reg.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Next we'll look at different metrics used to evaluate\n",
    "predictive performance. The word \"metric\" is used in the colloquial\n",
    "sense here, i.e. \"boiling the performance of the model down \n",
    "to a single number\". For brevity, the examples here will\n",
    "focus on metrics for supervised problems, but the sklearn\n",
    "package also comes with a host of scoring functions for \n",
    "unsupervised settings (e.g. the [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)\n",
    "for clustering evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "Regression problems involve predicting one or\n",
    "more real-valued target variables. Consequently,\n",
    "the metrics used try to quantify the answer to\n",
    "the question \"how close was the model?\". The \n",
    "notion of closeness depends on the problem and\n",
    "how you believe the error residuals (difference between\n",
    "prediction ($\\hat{y}$) and ground truth ($y$)) values are distributed, but\n",
    "the two most prolific ones are:\n",
    "\n",
    "- **Mean Squared Error (MSE)** ($\\frac{1}{N}\\sum_i^N{|\\hat{y}_i - y|^2}$) \n",
    "    which is equivalent to the sample variance of the error residuals.\n",
    "    This metric was use as a scoring function in the example\n",
    "    above.\n",
    "\n",
    "- **Mean Absolute Error (MAE)** ($\\frac{1}{N}\\sum_i^N{|\\hat{y}_i - y|}$)\n",
    "    which can be thought of as a more robust version of MSE, as it is\n",
    "    less sensitive to individual, extreme error residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.random.uniform(size=10000)\n",
    "# Compute y_pred as y_true + e, where e are error\n",
    "# residuals drawn from a centered normal distribution \n",
    "# with variance of 5\n",
    "y_pred = y_true + np.random.normal(0, np.sqrt(5), size=len(y_true))\n",
    "print('MSE: %.3f' % sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "print('MAE: %.3f' % sklearn.metrics.mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "\n",
    "In classification, we ask the model to predict\n",
    "whether a given datapoint belongs to a certain\n",
    "class or not. As opposed to regression problems,\n",
    "classification problems typically entail a \n",
    "\"hard decision\", meaning that a single prediction\n",
    "is either considered completely right or completely\n",
    "wrong. There also exist metrics (e.g. [cross entropy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) and [ROC AUC Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)) \n",
    "that account for  the degree of error, typically for classifiers that\n",
    "produce probability-like predictions, but in the \n",
    "examples below we will focus on metrics for hard decisions.\n",
    "\n",
    "**Accuracy** is perhaps the most straightforward\n",
    "classification metric. It answers the question:\n",
    "\"out of all the samples I was supposed to make \n",
    "predictions for, what fraction did I get right?.\n",
    "The example below uses sklearn's convenient \n",
    "`accuracy_score` function, but the behavior is \n",
    "equivalent to `np.mean(y_true == y_pred)` or \n",
    "$\\frac{1}{N} \\sum_i^N{\\mathbb{1}({y_i = \\hat{y}_i})}$,\n",
    "where $\\mathbb{1}(\\cdot)$ is an indicator function that\n",
    "is one if the predicate $\\cdot$ is true and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                  v  x  x  v  v\n",
    "y_true = np.array([0, 0, 0, 1, 1], dtype=np.float32)\n",
    "y_pred = np.array([0, 1, 1, 1, 1], dtype=np.float32)\n",
    "print('Accuracy: %.3f' % sklearn.metrics.accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** and **recall** answer two complementary questions:\n",
    "\n",
    "- Precision: of all the times the model predicted a\n",
    "    given class; what fraction of the times was it correct\n",
    "- Recall: of all the times the model should have predicted\n",
    "    a given class; what fraction of the times did it \n",
    "    \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "    \n",
    "For the binary classification example below, the class in\n",
    "question is the positive class (1). For multi-class prediction\n",
    "tasks you have to compute precision and recall for each class\n",
    "while considereing all classes not currently being evaluated\n",
    "as being the negative class (one-vs-all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0, 0, 0, 1, 1], dtype=np.float32)\n",
    "y_pred = np.array([0, 1, 1, 1, 1], dtype=np.float32)\n",
    "print('Precision: %.3f' % sklearn.metrics.precision_score(y_true, y_pred))\n",
    "print('Recall: %.3f' % sklearn.metrics.recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 Score** combines precision and recall into a \n",
    "single number. Mathematically, it can be formulated \n",
    "as $2 \\frac{precision * recall}{precision + recall}$.\n",
    "More intuitively, it can be thought of as how well the\n",
    "model balances precision and recall, proxied by a \n",
    "number proportional to the ratio between the area \n",
    "and boundary of a rectangle spanned by the two \n",
    "quantities. As with precision and recall, it can still \n",
    "only be calculated in a binary setting, meaning that a \n",
    "one-vs-all approach is needed when evaluating over \n",
    "multi-class prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0, 0, 0, 1, 1], dtype=np.float32)\n",
    "y_pred = np.array([0, 1, 1, 1, 1], dtype=np.float32)\n",
    "print('F1 score: %.3f' % sklearn.metrics.recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "# Plotting utilities \n",
    "sns.set_style('whitegrid')\n",
    "import cmat\n",
    "cmat.config.fig_facecolor = (1.0, 1.0, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Data\n",
    "\n",
    "We begin by downloading the Yeast Dataset from the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Yeast) \n",
    "of open datasets. This is a categorical (as opposed to binary) classification\n",
    "task where the goal is to assign proteins into one out of 10 classes.\n",
    "From the dataset metadata, we have the following description of each attribute.\n",
    "\n",
    " - Sequence Name: Accession number for the SWISS-PROT database \n",
    " - mcg: McGeoch's method for signal sequence recognition. \n",
    " - gvh: von Heijne's method for signal sequence recognition. \n",
    " - alm: Score of the ALOM membrane spanning region prediction program.  \n",
    " - mit: Score of discriminant analysis of the amino acid content of the N-terminal region (20 residues long) of mitochondrial and non-mitochondrial proteins. \n",
    " - erl: Presence of \"HDEL\" substring (thought to act as a signal for retention in the endoplasmic reticulum lumen). Binary attribute. \n",
    " - pox: Peroxisomal targeting signal in the C-terminus. \n",
    " - vac: Score of discriminant analysis of the amino acid content of vacuolar and extracellular proteins. \n",
    " - nuc: Score of discriminant analysis of nuclear localization signals of nuclear and non-nuclear proteins.\n",
    " \n",
    "This is a tabular dataset that comes as a text document where rows are separated by the \n",
    "standard newline `\\n` character and columns are separated by spaces.\n",
    "The Pandas csv parser can handle this format, but we need to specify the value separation\n",
    "pattern by using the regex `'\\s+'` (one or more spaces) as opposed to the default `','`.\n",
    "In addition, since the dataset comes without headers, we have to specify the column names\n",
    "manually as a list of strings.\n",
    "\n",
    "**Note**: Since we use a url to download the dataset straight into a DataFrame, this \n",
    "cell will not work in an off-line setting. It is done this way to keep this notebook\n",
    "portable, but a valid alternative would be to first download and save the dataset \n",
    "file to `/some/path` and then read it with `pd.read_csv('/some/path', ...)`.\n",
    "\n",
    "Finally we use the `.info()` method of the DataFrame to get a high-level summary\n",
    "of its content. The `Sequence Name` column and the `target` column take on string values,\n",
    "but the rest of the features are all numeric. Fortunately, none of the 1484 datapoints\n",
    "contain any missing/null values. If this was the case, we might have wanted to look\n",
    "into [imputation techniques](https://scikit-learn.org/stable/modules/impute.html) to\n",
    "fill in the blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data'\n",
    "columns = ['Sequence Name', 'mcg', 'gvh', 'alm', 'mit', 'erl', 'pox', 'vac', 'nuc', 'target']\n",
    "dataset = pd.read_csv(url, names=columns, sep='\\s+')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Distribution\n",
    "\n",
    "Next we'll have a look at the distribution of classes. The `value_counts()`\n",
    "method is used on the `target` column of the DataFrame to get a series containing\n",
    "the number of occurrences of each class. The series is the divided by the total\n",
    "size of the dataset to get the fractional representation of each class. The \n",
    "seaborn package is then used to generate a pretty barchart of the distribution,\n",
    "using the distribution index (values being counted) along the x-axis and \n",
    "the distribution values (relative frequencies) along the y-axis.\n",
    "\n",
    "The dataset is highly imbalanced. The `CYT` and `NUC` classes constitute around \n",
    "30% each, the `MIT` class around 15%, and the `ME3` class around 10%. The remaining \n",
    "~15% is distributed among the remaining 6 classes, with `ERL` being the least \n",
    "represented class (only 5 samples).\n",
    "\n",
    "Imbalanced target distributions pose a challenge for classification tasks.\n",
    "Many machine learning models, either implicitly or explicitly, assume \n",
    "a uniform/balanced target distribution, which can lead to poor performance\n",
    "when this is not the case, particularly when doing inference for samples of the \n",
    "underrepresented classes. Additionally, imbalanced class distributions \n",
    "can often lead to deceiving results when using certain metrics for \n",
    "evaluating predictive performance. For instance, consider the binary classification\n",
    "problem of $x \\rightarrow y$, $y \\in \\{0, 1\\}$. If $p(Y=1) = 0.99$, i.e. 99% of \n",
    "samples of $y$ take on the value of 1, then a classifier that deterministically\n",
    "maps all $x \\rightarrow 1$ will obtain an accuracy of 99%, which at a glance might\n",
    "look really impressive despite the fact that it doesn't use any of the information\n",
    "in $x$ to improve its posterior estimate over $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dist = dataset.target.value_counts() / len(dataset)\n",
    "sns.barplot(x=target_dist.index, y=target_dist.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplots with 2-and-2 features\n",
    "\n",
    "It is important to properly explore the data in order to\n",
    "make informed decisions around the choice of model and \n",
    "hyperparameters to use. Additionally, the dataset might have\n",
    "features that either contain little information and should\n",
    "be dropped or that could be transformed into a representation\n",
    "that is easier to work with for the model.\n",
    "For more inspiration and guidance on how this can be done,\n",
    "check out [this kaggle notebook](https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization).\n",
    "\n",
    "Here, we'll stick with a simple, yet fairly informative \n",
    "grid of plots generated by the `pairplot` function from seaborn. \n",
    "It generates scatter plots for each pair of features in \n",
    "the dataset, and a histogram for each variable along the \n",
    "diagonal. In addition, the data are color coded based on\n",
    "the target variable becuase we set the optional `hue`\n",
    "keyword argument. A few observations about the data include:\n",
    "\n",
    "- Except maybe for (gvh, mcg), there is relatively little \n",
    "  correlation between the features. If there is a lot of \n",
    "  correlation between variables, the dataset can often \n",
    "  be simplified with e.g. [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "- There appears to be a finite number of values for \n",
    "    erl (2) and pox(3). These variables could be encoded\n",
    "    as categorical variables without loosing any information\n",
    "    (except their relative order). However, the distribution\n",
    "    between the different categories appear to be highly\n",
    "    inbalanced, so simply dropping the featues might \n",
    "    also be an option.\n",
    "- The alm feature appears to do a good job at separating \n",
    "    the CYT (green) and ME3 (pink) classes. It could \n",
    "    potentially work well with simple models such as \n",
    "    logistic regression or linear SVM for this (isolated)\n",
    "    binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, hue='target', diag_kind='hist', plot_kws={'alpha': 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split\n",
    "\n",
    "In machine learning, we primarily care about out-of-sample performance, \n",
    "i.e., how well does the model generalize when applied to novel datapoints.\n",
    "It is, therefore, very common to split the dataset into (at least)\n",
    "two parts:\n",
    "\n",
    "- A training set to fit model and parameters\n",
    "- A testing set to estimate out-of-sample performance\n",
    "\n",
    "We start by separating the features from the categorical targets.\n",
    "The column called \"Sequence Name\" is also dropped, as it is just a unique\n",
    "identifier for each datapoint (could have been used as an index).\n",
    "The `train_test_split` function from the `model_selection`\n",
    "sub-package of sklearn is then used to partition the data \n",
    "into two disjoint sets. The sets are sampled randomly, but \n",
    "constrained by the `stratify` argument to ensure proportional\n",
    "representation of all classes across the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from targets \n",
    "X = dataset.drop(columns=['target', 'Sequence Name'])\n",
    "y = dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    # Data to split up\n",
    "    X, y,\n",
    "    # Fraction of samples in the train set (60%)\n",
    "    train_size=0.6, \n",
    "    # Shuffle before splitting, so that the sets are sampled randomly\n",
    "    shuffle=True,\n",
    "    # Random seed, for reproducibility \n",
    "    random_state=123,\n",
    "    # Stratify splits so that class distribution is preserved  \n",
    "    stratify=y\n",
    ")\n",
    "print('Training on %s samples' % len(X_train))\n",
    "print('Testing on %s samples' % len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Classification with Random Forest\n",
    "\n",
    "Without more deliberation, we'll try to train a classifier\n",
    "and see what happens. We will use a [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html),\n",
    "which is an ensemble model based on decision trees.\n",
    "It's key distinguishing property, compared to single-tree \n",
    "algorithms, is that it constructs an ensemble\n",
    "of trees based on different subsamples of both features\n",
    "and datapoints. During inference, the predictions from all\n",
    "the individual trees in the forest are aggregated into\n",
    "a single prediction which (assuming that the trees are\n",
    "reasonably uncorrelated) is expected to have a lower\n",
    "error than any of the individual trees.\n",
    "\n",
    "After constructing the classifier with default parameters (except for seed)\n",
    "and fitting it to the training data, a prediction is made across\n",
    "both the test set and the training set.\n",
    "The [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "function is then used to compute precision, recall, and f1-score \n",
    "for each class, as well as aggregates and accuracy across all the data.\n",
    "We also generate *confusion matrices* for both sets of predictions to \n",
    "get a better overview of which classes the model is more likely to conflate. \n",
    "\n",
    "It can be observed that the default RF classifiers manages\n",
    "to fit the training data quite well. In fact, it gets every\n",
    "single training sample right. The test set tells a different story.\n",
    "Only 61% of the predictions are correct, which is certainly \n",
    "better than random guessing, but not amazing. Additionally, performance\n",
    "across samples of the most underrepresented classes are abyssmal. \n",
    "The discrepency between the training set and the testing set\n",
    "could be an indication of a concept known as [overfitting](https://en.wikipedia.org/wiki/Overfitting),\n",
    "which is when increased in-sample performance comes at the cost\n",
    "of worse out-of-sample performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier with default parameters\n",
    "model = sklearn.ensemble.RandomForestClassifier(random_state=123) \n",
    "# Fit it to the training data \n",
    "model.fit(X_train, y_train)\n",
    "# Make in- and out-of-sample predictions\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "# Print out classification results and plot confusion matrices \n",
    "r_train = sklearn.metrics.classification_report(y_train, y_train_hat, zero_division=0)\n",
    "print('{0:->53}\\n{0:>25}Train{0:<22}\\n{0:->53}'.format(\"\")); print(r_train)\n",
    "r_test = sklearn.metrics.classification_report(y_test, y_test_hat, zero_division=0)\n",
    "print('{0:->53}\\n{0:>25}Test {0:<22}\\n{0:->53}'.format(\"\")); print(r_test)\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(16,6), ncols=2)\n",
    "cmat.ConfusionMatrix.create(y_train, y_train_hat).plot(ax=ax1, title='Train')\n",
    "cmat.ConfusionMatrix.create(y_test, y_test_hat).plot(ax=ax2, title='Test');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimization with Optuna\n",
    "\n",
    "The out-of-sample performance of the approach above leaves\n",
    "some to be desired. One possible improvement could be to have another\n",
    "look at the data and e.g. try to [reduce the number of features](https://scikit-learn.org/stable/modules/feature_selection.html),\n",
    "which, assuming that some of the features contain more noise than signal,\n",
    "should have a regularizing effect on the classifier and thus\n",
    "reduce the gap between train and test set error. Alternatively, a \n",
    "different choice of model family could be explored, such as \n",
    "[softmax regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html),\n",
    "[KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html),\n",
    "or [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "For brevity, we'll stick to the Random Forest Classifier here, but \n",
    "try changing its behavior by tuning hyperparameters. \n",
    "\n",
    "The Random Forest Classifier has several hyperparameters that affects\n",
    "how it is trained. Most of them place constraints on how the decision\n",
    "trees are built and limits the expressiveness of the model, which in \n",
    "turn contributes to inductive bias. Other hyperparameters control how\n",
    "data for each tree are sampled, which in turn affects how similar each\n",
    "tree is likely to be, and thus, how unorrelated we can expect the errors\n",
    "to be. The full set of hyperparameters for the sklearn implementation \n",
    "used here can be found in [its documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html),\n",
    "but the example here will focus on the following three:\n",
    "\n",
    "- **n_estimators** - the number of decision trees used in the forest. The expressiveness/capacity of the model increases with this value.\n",
    "- **max_depth** - the maximum depth of each tree. The expressiveness/capacity of the model also increases with this parameter.\n",
    "- **class_weight** - class-based weighting of samples when forming tree nodes. This parameter can take 4 different values:\n",
    "    - None (default) - in which case the algorithm will treat all samples the same\n",
    "    - balanced - in which case samples will be weighted inversly proportionally to their representation in the dataset\n",
    "    - balanced_subsample - in which case samples will be weighted inversly proportionally to their representation in the bootstrap samples used to form a single tree\n",
    "    - dict/list - in which case custom weights given by the input will be used (we will ignore this option)\n",
    "\n",
    "The hyperparameters will be optimized by a package called [Optuna](https://optuna.readthedocs.io/en/stable/),\n",
    "which permits optimization of arbitrary stochastic, black-box functions.\n",
    "In this case, the black-box function is the mapping from hyperparameters\n",
    "to average K-fold cross-validation score, which is implemented in \n",
    "the `objective` function below.\n",
    "To do this, optuna runs multiple experiments with different choices of \n",
    "hyperparameters while maintaining an estimate of how choices affect\n",
    "the resulting performance, and suggesting new values that are expected\n",
    "to maximize score based on this belief. \n",
    "The example below uses the (default) TPE algorithm to control this process,\n",
    "which the interested reader can find more information about in section 4\n",
    "of [this paper](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf).\n",
    "Analogous libraries to optuna include [hyperopt](http://hyperopt.github.io/hyperopt/) \n",
    "and [nevergrad](https://facebookresearch.github.io/nevergrad/), and\n",
    "more brute-force alternatives include [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "and (of course) manual trial and error by the researcher.\n",
    "\n",
    "**Side Note:** the following cell is quite long. In a structured project,\n",
    "it might be prudent to move longer snippets of code into separate python\n",
    "files and import them into the notebook as that will; (A) make it easier\n",
    "to version control with git, (B) permit it to be shared between notebooks,\n",
    "and (C) make the notebook tidier. We'll take this opportunity to demonstrate\n",
    "how this can be further streamlined with the *autoreload* functionality\n",
    "in the cells bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile optimize.py \n",
    "# ^ IPython magic for saving content of current cell to file \n",
    "# Have a look in the folder this notebook is running from\n",
    "\n",
    "import sklearn.ensemble \n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import optuna \n",
    "\n",
    "def hyperopt_random_forest(X, y, scoring, n_trials=100, n_splits=3, verbose=True, seed=None):\n",
    "    \"\"\"\n",
    "    Optimizes hyperparameters for a Random Forest Classifier.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        X: an MxN-dimensional design matrix with features\n",
    "        y: an M-dimensional vector of categorical targets\n",
    "        scoring: a string with the name of which scoring function to use. \n",
    "            See https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "        n_trials: how many trials to run the optimizer for \n",
    "        n_splits: K in K-Fold cross validation\n",
    "        verbose: whether to enable Optuna logging \n",
    "        seed: random seed (int or None) to use\n",
    "        \n",
    "    Returns:\n",
    "        An optuna study object with information about the optimization process\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create random state for reproducibility\n",
    "    random = np.random.RandomState(seed)\n",
    "    \n",
    "    # Objective function that will be called each optimization iteration \n",
    "    def objective(trial):\n",
    "        # Setup model, allowing the optimization \"trial\" object\n",
    "        # to select hyperparameters from predefined ranges\n",
    "        model = sklearn.ensemble.RandomForestClassifier(\n",
    "            # Sample how many trees to use between 1 and 100\n",
    "            n_estimators=trial.suggest_int('n_estimators', 1, 100),\n",
    "            # Sample maximum depth of each tree between 1 and 20\n",
    "            max_depth=trial.suggest_int('max_depth', 1, 20),\n",
    "            # Sample class weighting strategy among the 3 options\n",
    "            class_weight=trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "            # Pass on seeded random state for reproducibility \n",
    "            random_state=random\n",
    "        )\n",
    "        \n",
    "        # Setup cross validation scheme, use StratifiedKFold\n",
    "        # to ensure proportional class balance in train and test set\n",
    "        cv = sklearn.model_selection.StratifiedKFold(\n",
    "            # Do an n_split\n",
    "            n_splits=n_splits, \n",
    "            # Pass on seeded random state for reproducibility\n",
    "            random_state=random,\n",
    "            # Sample randomly \n",
    "            shuffle=True,\n",
    "        )\n",
    "        \n",
    "        # Compute cross-validation score for the model using the given performance measure\n",
    "        return sklearn.model_selection.cross_val_score(model, X, y, scoring=scoring, cv=cv).mean()\n",
    "    \n",
    "    # Set verbosity level for optuna \n",
    "    if verbose:\n",
    "        optuna.logging.set_verbosity(20)\n",
    "    else:\n",
    "        optuna.logging.set_verbosity(0)\n",
    "    \n",
    "    # Create optuna study and optimize `objective` function for \n",
    "    # the given number of iterations \n",
    "    study = optuna.create_study(\n",
    "        sampler=optuna.samplers.TPESampler(seed=seed),\n",
    "        direction='maximize',\n",
    "    )\n",
    "    # Run optimizer. This will call the `objective` function `n_trials`.\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External Code Reloading\n",
    "\n",
    "Even though the function defined in the cell above can be used as it, we \n",
    "will load it from a file instead to show how the autoreload module can be used.\n",
    "By default, code imported from external python modules will only be loaded\n",
    "once, meaning any changes done to it afterwards will not be reflected in\n",
    "the python process it was imported into. This is particularly annoying when\n",
    "working with jupyter notebooks, were it is common to let the process keep\n",
    "running for long periods of time.\n",
    "\n",
    "This problem can be fixed manually by using the [reload function](https://www.geeksforgeeks.org/reloading-modules-python/),\n",
    "but in IPython (which is what is running behind the scenes in jupyter), things \n",
    "can be made even easier with the [autoreload extension](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html).\n",
    "The autoreload extension allows imported modules to be automatically reloaded \n",
    "and is enabled by running the following IPython \"[magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html)\" \n",
    "command (typically at the top of the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimizer Execution\n",
    "\n",
    "Once the autoreload extension is loaded, we can activate it with the `%autoreload` \n",
    "magic keyword. This will ensure that whenever this cell is run, all the imported\n",
    "modules will be reloaded. Here we will import the `optimize.py` module that was\n",
    "generated by the `%writefile` keyword two cells above. Since autoreloading is \n",
    "enabled, any changes to that file will automatically applied whenever running\n",
    "this cell. Note: we could also have used the `%autoreload 2` keyword, in which\n",
    "case the module would be reloaded every single time any code was run anywhere \n",
    "in this notebook.\n",
    "\n",
    "With autoreloading out of the way, lets get back to the main problem at hand;\n",
    "optimizing the hyperparameters of the Random Forest classifier. We will run \n",
    "the optimizer for a total of 50 iterations and use `f1_macro` (unweighted \n",
    "average of f1-scores across all classes) as the optimization target. Only \n",
    "the training data will be passed to the optimization function. This entails\n",
    "that the cross-validation procedure used will further partition the training\n",
    "set K times, but never interact with the test set. This is important to prevent\n",
    "risking accidentally overfitting the hyperparameters to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# Import the python module. The %autoreload cell magic will\n",
    "# make sure that the imported module is up-to date every time \n",
    "# this cell is run \n",
    "import optimize\n",
    "study = optimize.hyperopt_random_forest(X_train, y_train, 'f1_macro', n_trials=50, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from the Hyperparameter Optimizer\n",
    "\n",
    "Having run the hyperparameter optimizer, we will now have a look at the results.\n",
    "The `study` object used in the optimizer contains all the information about \n",
    "the search, including the history of suggested parameters and the resulting score \n",
    "that can be accessed as a DataFrame by using the `trials_dataframe()` method.\n",
    "The parameters and resulting score of the most successful trial are also\n",
    "respectively available under the attributes `best_value` and `best_params`.\n",
    "\n",
    "We then generate plots to visualize the impact of the parameters. Since the \n",
    "search included three distinct parameters, it would be natural to create three plots,\n",
    "but since the `class_weight` parameter only have three distinct values it has \n",
    "been color coded instead. It can be seen that the most impactful hyperparameters\n",
    "is the balancing strategy; no balancing is consistently bad, and general balancing\n",
    "appears to be slighly better than balanced subsampling. The max depth and number \n",
    "of estimators have a smaller impact on performance, but there appear to be slight\n",
    "peaks around the parameters of the most successful trial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = study.trials_dataframe()\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(16,5), ncols=2)\n",
    "sns.scatterplot(x=df.params_max_depth, y=df.value, hue=df.params_class_weight.replace({None:'None'}).to_list(), ax=ax1)\n",
    "sns.scatterplot(x=df.params_n_estimators, y=df.value, hue=df.params_class_weight.replace({None:'None'}).to_list(), ax=ax2)\n",
    "print(f'Best Score: {study.best_value :.3f}')\n",
    "print(f'Best Params: {study.best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with Optimized Random Forest\n",
    "\n",
    "Armed with a new, slighly better set of parameters that were optimized for \n",
    "macro f1-score, we rerun the classifier over the entire training set and \n",
    "evaluate it on the test set. Starting with the macro statistics, it can\n",
    "be seen that precision, recall, and f1-score have been drastically improved\n",
    "without taking a hit to the overal accuraccy. Performance on the least \n",
    "represented class, `ERL`, has been drastically improved from zero precision \n",
    "and recall to 100%. However, it is important to keep in mind that this class \n",
    "only have a support of 2 in the test set, so although it is nice to see it \n",
    "being addressed, it is difficult to confidently say anything about whether \n",
    "this classifier is well suited for identifying future instances of it.\n",
    "Additionally, since the `ERL` class constitutes 1/10th of all the classes,\n",
    "just getting these two predictions right is actually enough to raise the \n",
    "unweighted macro precision by 0.1, which alone can explain the full hike in\n",
    "precision from 0.48 to 0.58."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier with default parameters\n",
    "model = sklearn.ensemble.RandomForestClassifier(random_state=124, **study.best_params) \n",
    "# Fit it to the training data \n",
    "model.fit(X_train, y_train)\n",
    "# Make in- and out-of-sample predictions\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "# Print out classification results and plot confusion matrices \n",
    "r_train = sklearn.metrics.classification_report(y_train, y_train_hat, zero_division=0)\n",
    "print('{0:->53}\\n{0:>25}Train{0:<22}\\n{0:->53}'.format(\"\")); print(r_train)\n",
    "r_test = sklearn.metrics.classification_report(y_test, y_test_hat, zero_division=0)\n",
    "print('{0:->53}\\n{0:>25}Test {0:<22}\\n{0:->53}'.format(\"\")); print(r_test)\n",
    "# print('\\n'.join(map(f'{\"\":>15}'.join, zip(r_train.split('\\n'), r_test.split('\\n')))))\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(16,6), ncols=2)\n",
    "cmat.ConfusionMatrix.create(y_train, y_train_hat).normalize('recall').plot(ax=ax1, title='Train')\n",
    "cmat.ConfusionMatrix.create(y_test, y_test_hat).normalize('recall').plot(ax=ax2, title='Test');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
